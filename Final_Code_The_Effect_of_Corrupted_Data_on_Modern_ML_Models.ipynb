{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80_GCu4UDRDp"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NhH8HUhZsND"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install sklearn\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE2KXL2hDVqK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SLe5NBRGNt8"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D, Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, MaxPooling2D, ZeroPadding2D, Concatenate, Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling2D\n",
        "from keras.regularizers import l2, l1, l1_l2\n",
        "from keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3chJNnkKRfS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTFV2oCWAkAV"
      },
      "source": [
        "##Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QoKpHeO_naA"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist, cifar100, cifar10, fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBnCAqEHWMs5"
      },
      "outputs": [],
      "source": [
        "def generate_datasets(dataset_name, flat = False, normalize = True):\n",
        "  # Download dataset\n",
        "  if dataset_name == \"mnist\":\n",
        "    (x_train_val, y_train_val), (x_test, y_test) = mnist.load_data()\n",
        "    output_shape = 10\n",
        "  if dataset_name == \"cifar100\":\n",
        "    (x_train_val, y_train_val), (x_test, y_test) = cifar100.load_data()\n",
        "    output_shape = 100\n",
        "  if dataset_name == \"cifar10\":\n",
        "    output_shape = 10\n",
        "    (x_train_val, y_train_val), (x_test, y_test) = cifar10.load_data()\n",
        "  if dataset_name == \"fashion_mnist\":\n",
        "    output_shape = 10\n",
        "    (x_train_val, y_train_val), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "  # Create validation set\n",
        "  x_train, x_valid, y_train, y_valid = train_test_split(x_train_val, y_train_val, test_size=0.2, shuffle= True, random_state= 1) #123 #1234\n",
        "\n",
        "  # Size\n",
        "  print(\"Dataset size:\" + str(y_train_val.shape[0] + y_test.shape[0]))\n",
        "  print(\"Training + Validation set size:\" + str(y_train_val.shape[0]) + str(\" -- \") + \"Training set size:\" + str(y_train.shape[0]) + str(\"  \") + \"Validation set size:\" + str(y_valid.shape[0]))\n",
        "  print(\"Test set size:\" + str(y_test.shape[0]))\n",
        "\n",
        "  if not isinstance(y_train[0], np.uint8):\n",
        "    y_train = np.array([x.tolist()[0] for x in y_train])\n",
        "    y_valid = np.array([x.tolist()[0] for x in y_valid])\n",
        "    y_test = np.array([x.tolist()[0] for x in y_test])\n",
        "\n",
        "\n",
        "  print(\"Samples per label - total - training - validation - test:\")\n",
        "  all_labels = list(y_train) + list(y_valid) + list(y_test)\n",
        "  for label in set(all_labels):\n",
        "    print(\"Label:\" + str(label) + \" -- \" + \"Samples total:\" + str(all_labels.count(label)) + \" -- \" + \"Samples training:\" + str(list(y_train).count(label)) + \" -- \" + \"Samples validation:\" + str(list(y_valid).count(label)) + \" -- \" + \"Samples test:\" + str(list(y_test).count(label)))\n",
        "\n",
        "  # Reshape\n",
        "  if flat == True:\n",
        "    shape_list = list(x_train.shape)\n",
        "    input_shape = np.prod(shape_list[1:])\n",
        "    train_data = x_train.reshape((-1, input_shape))\n",
        "    validation_data = x_valid.reshape((-1, input_shape))\n",
        "    test_data = x_test.reshape((-1, input_shape))\n",
        "  else:\n",
        "    if len(x_train.shape[1:]) == 3:\n",
        "      pass\n",
        "    else:\n",
        "      x_train = np.expand_dims(x_train, axis=-1)\n",
        "      x_valid = np.expand_dims(x_valid, axis=-1)\n",
        "      x_test  = np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "      x_train = np.repeat(x_train, 3, axis=-1)\n",
        "      x_valid = np.repeat(x_valid, 3, axis=-1)\n",
        "      x_test = np.repeat(x_test, 3, axis=-1)\n",
        "\n",
        "    train_data = x_train\n",
        "    validation_data = x_valid\n",
        "    test_data = x_test\n",
        "\n",
        "  input_shape = tuple(train_data.shape[1:])\n",
        "\n",
        "  # Normalize\n",
        "  if normalize == True:\n",
        "    train_data = train_data/255\n",
        "    validation_data = validation_data/255\n",
        "    test_data = test_data/255\n",
        "\n",
        "  # Return train, validation, test sets\n",
        "  return train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3IVyDGWDYCm"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB4BSw5DvGQh"
      },
      "outputs": [],
      "source": [
        "def instantiate_model(model_name, dataset_name, input_shape, output_shape):\n",
        "  metrics = ['acc']\n",
        "  if model_name == \"Logistic regression\":\n",
        "    input = Input(shape=(np.prod(list(input_shape)), ))\n",
        "    dense = Dense(output_shape)(input)\n",
        "    dense = BatchNormalization()(dense)\n",
        "    output = Activation('sigmoid')(dense)\n",
        "    model = Model(input, output)\n",
        "    model.compile(optimizer= 'sgd', loss= 'sparse_categorical_crossentropy', metrics=metrics)\n",
        "    return model \n",
        "  \n",
        "  if \"Resnet\" in model_name:\n",
        "    if dataset_name == \"cifar10\" or dataset_name == \"cifar100\":\n",
        "      size = (7,7)\n",
        "      optimizer = 'sgd'\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      size = (8,8)\n",
        "      optimizer = 'adam'\n",
        "\n",
        "    input = Input(shape=input_shape)\n",
        "    x = UpSampling2D(size=size)(input)\n",
        "\n",
        "    if \"50v1\" in model_name:\n",
        "      x = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling=None)(x)\n",
        "    elif \"50v2\" in model_name:\n",
        "      x = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling=None)(x)\n",
        "    \n",
        "    if \"101v1\" in model_name:\n",
        "      x = tf.keras.applications.ResNet101(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling=None)(x)\n",
        "    elif \"101v2\" in model_name:\n",
        "      x = tf.keras.applications.ResNet101V2(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling=None)(x)\n",
        "\n",
        "    if \"152v1\" in model_name:\n",
        "      x = tf.keras.applications.ResNet152(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling=None)(x)\n",
        "    elif \"152v2\" in model_name:\n",
        "      x = tf.keras.applications.ResNet152V2(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling=None)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    output = tf.keras.layers.Dense(output_shape, activation=\"softmax\")(x)\n",
        "    model = Model(input, output)\n",
        "\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      for layer in model.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "\n",
        "    model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics=metrics)\n",
        "    return model\n",
        "\n",
        "  if model_name == \"Alexnet\": # img_shape=(224, 224, 3), n_classes=10\n",
        "    if dataset_name == \"cifar10\" or dataset_name == \"cifar100\":\n",
        "      size = (7,7)\n",
        "      optimizer = 'sgd'\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      size = (8,8)\n",
        "      optimizer = 'adam'\n",
        "\n",
        "    input = Input(shape=input_shape)\n",
        "    x = UpSampling2D(size=size)(input)\n",
        "    x = Conv2D(96, (11, 11), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    \n",
        "    x= Conv2D(256, (5, 5), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(512, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(1024, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(1024, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(3072)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Layer 7\n",
        "    x = Dense(4096)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Layer 8\n",
        "    x = Dense(output_shape)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    output = Activation('softmax')(x)\n",
        "\n",
        "    model = Model(input, output)\n",
        "    model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics=metrics)\n",
        "    return model\n",
        "\n",
        "  if \"VGG\" in model_name:\n",
        "    if dataset_name == \"cifar10\" or dataset_name == \"cifar100\":\n",
        "      size = (7,7)\n",
        "      optimizer = 'sgd'\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      size = (8,8)\n",
        "      optimizer = 'adam'\n",
        "\n",
        "    input = Input(shape=input_shape)\n",
        "    x = UpSampling2D(size=size)(input)\n",
        "\n",
        "    if \"16\" in model_name: \n",
        "      x = tf.keras.applications.VGG16(include_top=False, weights=\"imagenet\", input_shape=(224,224,3), pooling=None)(x)\n",
        "    else: \n",
        "      x = tf.keras.applications.VGG19(include_top=False, weights=\"imagenet\", input_shape=(224,224,3), pooling=None)(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    output = tf.keras.layers.Dense(output_shape, activation=\"softmax\")(x)\n",
        "    model = Model(input, output)\n",
        "\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      for layer in model.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "\n",
        "    model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics=metrics)\n",
        "    return model\n",
        "\n",
        "  if \"Densenet\" in model_name:\n",
        "    if dataset_name == \"cifar10\" or dataset_name == \"cifar100\":\n",
        "      size = (7,7)\n",
        "      optimizer = 'sgd'\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      size = (8,8)\n",
        "      optimizer = 'adam'\n",
        "\n",
        "    input = Input(shape=input_shape)\n",
        "    x = UpSampling2D(size=size)(input)\n",
        "\n",
        "    if \"121\" in model_name:\n",
        "      x = tf.keras.applications.DenseNet121(include_top=False, weights=\"imagenet\", input_shape=(224,224,3), pooling=None)(x)\n",
        "    elif \"169\" in model_name:\n",
        "      x = tf.keras.applications.DenseNet169(include_top=False, weights=\"imagenet\", input_shape=(224,224,3), pooling=None)(x)\n",
        "    else:\n",
        "      x = tf.keras.applications.DenseNet201(include_top=False, weights=\"imagenet\", input_shape=(224,224,3), pooling=None)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    output = tf.keras.layers.Dense(output_shape, activation=\"softmax\")(x)\n",
        "    model = Model(input, output)\n",
        "\n",
        "    if dataset_name == \"mnist\" or dataset_name == \"fashion_mnist\":\n",
        "      for layer in model.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "\n",
        "    model.compile(optimizer= optimizer, loss= 'sparse_categorical_crossentropy', metrics=metrics)\n",
        "    return model\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiOgSY0p4do8"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbNU21_u4dRC"
      },
      "outputs": [],
      "source": [
        "def train_model(model_name, dataset_name, batch_size, epochs, flat, normalize):\n",
        "  # Generate datasets\n",
        "  train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape= generate_datasets(dataset_name, flat, normalize)\n",
        "\n",
        "  # Instantiate model\n",
        "  model = instantiate_model(model_name, dataset_name, input_shape, output_shape)\n",
        "  \n",
        "  # Instantiate early stopping\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "  \n",
        "  # Train model \n",
        "  history = model.fit(train_data, y_train, batch_size = batch_size, epochs= epochs, validation_data = (validation_data, y_valid), callbacks=[stop_early])\n",
        "\n",
        "  # Evaluate model\n",
        "  score = model.evaluate(test_data, y_test, verbose = 0) \n",
        "  print('Test accuracy:', score[1])\n",
        "  print('Test loss:', score[0])\n",
        "\n",
        "  # Confusion matrix results \n",
        "  y_prediction = model.predict(test_data)\n",
        "  y_prediction = np.argmax(y_prediction, axis = 1)\n",
        "  result = confusion_matrix(y_test, y_prediction)\n",
        "  \n",
        "  print(\"The confusion matrix:\")\n",
        "  print(*result, sep=\"\\n\")\n",
        "  model_classes = list(set(sorted([x for x in y_test])))\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=result, display_labels= model_classes)\n",
        "\n",
        "  disp.plot()\n",
        "  plt.savefig(model_name + \"_\" + dataset_name, dpi = 4096)\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEpFJAxm8WL3"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Logistic regression\", \"mnist\", 128, 30, flat = True, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inX_yvfq_XG8"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Resnet50v1\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ44UCB4_ff_"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Resnet50v2\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMh38PsF_hRb"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Resnet101v1\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVcAMRTq_lIu"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Resnet101v2\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOEQ0nt8_mO9"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Resnet152v1\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L63lXRl0_npd"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Resnet152v2\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEzzBcNr_pyO"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"VGG16\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTv7mdI0_toP"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"VGG19\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrzC_Yms_vc-"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Densenet121\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZLVopRZ_y4L"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Densenet169\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPhg2KSc_1cx"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Densenet201\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrgBpHBd_29K"
      },
      "outputs": [],
      "source": [
        "model = train_model(\"Alexnet\", \"mnist\", 128, 30, flat = False, normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZgvteS2OUIE"
      },
      "source": [
        "## Corruption mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ2BKNZAOTzQ"
      },
      "outputs": [],
      "source": [
        "def corrupt_labels(corruption_type, train_labels, validation_labels, test_labels, total_labels, percentage_random_labels = None):\n",
        "\n",
        "  if corruption_type == \"shift_label\":\n",
        "    labels = list(range(total_labels))\n",
        "    np.random.seed(73)\n",
        "    init_label = np.random.choice(labels) \n",
        "    labels.remove(init_label)\n",
        "    new_label = np.random.choice(labels)\n",
        "    train_labels = np.array([new_label if el == init_label else el for el in train_labels])\n",
        "    #validation_labels = np.array([new_label if el == init_label else el for el in validation_labels])\n",
        "    print(\"Corruption type: Shift-labels where:\" + str(init_label) + \" was replaced by:\" + str(new_label))\n",
        "\n",
        "  if corruption_type == \"random_label\":\n",
        "    no_train_labels = len(train_labels)\n",
        "    no_valid_labels = len(validation_labels)\n",
        "    \n",
        "    random.seed(42)\n",
        "    train_corrupted_indices = random.sample(list(range(no_train_labels)), int(no_train_labels * percentage_random_labels))\n",
        "    np.random.seed(81)\n",
        "    train_corrupted_labels = np.random.choice(list(range(total_labels)), size = no_train_labels, replace = True)\n",
        "    new_train_labels = np.array([train_corrupted_labels[el] if el in train_corrupted_indices else train_labels[el] for el in list(range(no_train_labels))])\n",
        "\n",
        "    random.seed(53)\n",
        "    valid_corrupted_indices = random.sample(list(range(no_valid_labels)), int(no_valid_labels * percentage_random_labels))\n",
        "    np.random.seed(49)\n",
        "    valid_corrupted_labels = np.random.choice(list(range(total_labels)), size = no_valid_labels, replace = True)\n",
        "    new_valid_labels = np.array([valid_corrupted_labels[el] if el in valid_corrupted_indices else validation_labels[el] for el in list(range(no_valid_labels))])\n",
        "\n",
        "    train_labels = new_train_labels\n",
        "    validation_labels = new_valid_labels\n",
        "    \n",
        "    print(\"Corruption type: Random-label where:\" + str(percentage_random_labels) + \" percent of labels were replaced by random values\")\n",
        "\n",
        "  if corruption_type == \"random_map\":\n",
        "    initial_labels = list(range(total_labels))\n",
        "    \n",
        "    random.seed(72)\n",
        "    shuffled_labels = list(range(total_labels))\n",
        "    random.shuffle(shuffled_labels)\n",
        "\n",
        "    no_train_labels = len(train_labels)\n",
        "    no_valid_labels = len(validation_labels)\n",
        "    \n",
        "    random.seed(42)\n",
        "    train_corrupted_indices = random.sample(list(range(no_train_labels)), int(no_train_labels * percentage_random_labels))\n",
        "    new_train_labels = np.array([shuffled_labels[train_labels[el]] if el in train_corrupted_indices else train_labels[el] for el in list(range(no_train_labels))])\n",
        "\n",
        "    random.seed(53)\n",
        "    valid_corrupted_indices = random.sample(list(range(no_valid_labels)), int(no_valid_labels * percentage_random_labels))\n",
        "    new_valid_labels = np.array([shuffled_labels[validation_labels[el]] if el in valid_corrupted_indices else validation_labels[el] for el in list(range(no_valid_labels))])\n",
        "\n",
        "    train_labels = new_train_labels\n",
        "    validation_labels = new_valid_labels\n",
        "    \n",
        "    print(\"Corruption type: Random-map where:\" + str(percentage_random_labels) + \" percent of labels were replaced according to the random map\")\n",
        "\n",
        "  return train_labels, validation_labels, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updated training procedure"
      ],
      "metadata": {
        "id": "UL4dw0OSqN7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_corruption(model_name, dataset_name, batch_size, epochs, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape):\n",
        "\n",
        "  # Instantiate model\n",
        "  model = instantiate_model(model_name, dataset_name, input_shape, output_shape)\n",
        "  \n",
        "  # Instantiate early stopping\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "  \n",
        "  # Train model \n",
        "  history = model.fit(train_data, y_train, batch_size = batch_size, epochs= epochs, validation_data = (validation_data, y_valid), callbacks=[stop_early])\n",
        "\n",
        "  # Evaluate model\n",
        "  score = model.evaluate(test_data, y_test, verbose = 0) \n",
        "  print('Test accuracy:', score[1])\n",
        "  print('Test loss:', score[0])\n",
        "\n",
        "  # Confusion matrix results \n",
        "  y_prediction = model.predict(test_data)\n",
        "  y_prediction = np.argmax(y_prediction, axis = 1)\n",
        "  result = confusion_matrix(y_test, y_prediction)\n",
        "  print(y_test)\n",
        "  print(y_prediction)\n",
        "  \n",
        "  print(\"The confusion matrix:\")\n",
        "  print(*result, sep=\"\\n\")\n",
        "  model_classes = list(set(sorted([x for x in y_test])))\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=result, display_labels= model_classes)\n",
        "\n",
        "  disp.plot()\n",
        "  plt.savefig(model_name + \"_\" + dataset_name, dpi = 1024)\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "xRaFWOH2qL-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corruption_type = \"random_label\"\n",
        "dataset_name = \"mnist\"\n",
        "percentage_random_labels = 0.1\n",
        "\n",
        "# Generate datasets flat\n",
        "train_data_f, train_labels, validation_data_f, validation_labels, test_data_f, test_labels, input_shape_f, output_shape_f = generate_datasets(dataset_name, flat = True, normalize = True)\n",
        "print(train_labels[:200])\n",
        "\n",
        "# Generate datasets normal\n",
        "train_data, train_labels, validation_data, validation_labels, test_data, test_labels, input_shape, output_shape= generate_datasets(dataset_name, flat = False, normalize = True)\n",
        "print(train_labels[:200])\n",
        "\n",
        "# Adding the corruption strategy\n",
        "y_train, y_valid, y_test = corrupt_labels(corruption_type, train_labels, validation_labels, test_labels, output_shape, percentage_random_labels)\n",
        "print(y_train[:200])"
      ],
      "metadata": {
        "id": "qml_8dUWqRwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Logistic regression\", \"mnist\", 128, 30, train_data_f, y_train, validation_data_f, y_valid, test_data_f, y_test, input_shape_f, output_shape_f)"
      ],
      "metadata": {
        "id": "PStjC9EgqYYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Resnet50v1\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "YP9U6yioqY_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Resnet50v2\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "c4XrtbAVqcQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Resnet101v1\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "FWWSoFGsqceR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Resnet101v2\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "2t2u2ggqqcgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Resnet152v1\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "zKy_nY7Aqciz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Resnet152v2\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "p1ri9NaFqclB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"VGG16\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "uY7MycAOqcnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"VGG19\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "u0sx0-GKqcpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Densenet121\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "pnETrab1qoNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Densenet169\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "HRu-ZOiqqoO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Densenet201\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "Ph1SXaT5qoSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model_with_corruption(\"Alexnet\", \"mnist\", 128, 30, train_data, y_train, validation_data, y_valid, test_data, y_test, input_shape, output_shape)"
      ],
      "metadata": {
        "id": "zNz1ZJmYqrzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}